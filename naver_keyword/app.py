from fastapi import FastAPI, HTTPException, status
from fastapi.responses import JSONResponse
from scraper import Scraper, ScraperException
from selenium_pool import get_driver_pool, cleanup_driver_pool
import re
from urllib.parse import unquote
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading
import traceback
from requests.exceptions import RequestException
import logging
from typing import Dict, Any
import atexit

app = FastAPI(
    title="Naver Keyword Scraper",
    description="ë„¤ì´ë²„ í‚¤ì›Œë“œ ìŠ¤í¬ë˜í•‘ API - ì•ˆì •ì„±, ì¬ì‹œë„ ë¡œì§, ì†ë„ ìµœì í™” (ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ Selenium ì‚¬ìš©)",
    version="2.1.1"
)

# ThreadPoolExecutor ì„¤ì • (CPU ì½”ì–´ * 2)
executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="scraper_worker")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - [%(threadName)s] - %(message)s"
)

logger = logging.getLogger('uvicorn')

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ë“œë¼ì´ë²„ í’€ ì •ë¦¬
@app.on_event("shutdown")
async def shutdown_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    logger.info("Application shutting down, cleaning up driver pool...")
    cleanup_driver_pool()
    logger.info("Driver pool cleanup completed")

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œì—ë„ ì •ë¦¬
atexit.register(cleanup_driver_pool)

def naver_related(keywords: str) -> Dict[str, Any]:
    """ì—°ê´€ê²€ìƒ‰ì–´ ìŠ¤í¬ë˜í•‘ (ë™ê¸° í•¨ìˆ˜)
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
        
    Returns:
        ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        
    Raises:
        ScraperException: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ
    """
    logger.info(f"[WORKER] Starting naver_related for: {keywords}")
    scraper = Scraper()
    result = scraper.scrape_naver_related(query=keywords)
    logger.info(f"[WORKER] Completed naver_related for: {keywords}, found {len(result['result'])} results")
    return result

def naver_popular(keywords: str) -> Dict[str, Any]:
    """ì¸ê¸°ì£¼ì œ ìŠ¤í¬ë˜í•‘ (ë™ê¸° í•¨ìˆ˜)
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
        
    Returns:
        ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        
    Raises:
        ScraperException: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ
    """
    logger.info(f"[WORKER] Starting naver_popular for: {keywords}")
    scraper = Scraper()
    result = scraper.scrape_naver_popular(query=keywords)
    logger.info(f"[WORKER] Completed naver_popular for: {keywords}, found {len(result['result'])} results")
    return result

def naver_together(keywords: str) -> Dict[str, Any]:
    """í•¨ê»˜ì°¾ì€ í‚¤ì›Œë“œ ìŠ¤í¬ë˜í•‘ (ë™ê¸° í•¨ìˆ˜)
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
        
    Returns:
        ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        
    Raises:
        ScraperException: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ
    """
    logger.info(f"[WORKER] Starting naver_together for: {keywords}")
    scraper = Scraper()
    result = scraper.scrape_naver_together(query=keywords)
    logger.info(f"[WORKER] Completed naver_together for: {keywords}, found {len(result['result'])} results")
    return result

@app.get(
    "/search/naver_related",
    summary="ë„¤ì´ë²„ ì—°ê´€ê²€ìƒ‰ì–´ ìŠ¤í¬ë˜í•‘",
    description="ë„¤ì´ë²„ ê²€ìƒ‰ ì—°ê´€ê²€ìƒ‰ì–´ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ HTTP 500 ì—ëŸ¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_model=None
)
async def related(keywords: str):
    """ë„¤ì´ë²„ ì—°ê´€ê²€ìƒ‰ì–´ ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
        
    Returns:
        {'keyword': str, 'result': [{'rank': int, 'keyword': str}, ...]}
        
    Raises:
        HTTPException: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ 500 ì—ëŸ¬
    """
    logger.info(f"[API] Received request for naver_related: {keywords}")
    
    try:
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(executor, naver_related, keywords)
        logger.info(f"[API] Successfully completed naver_related: {keywords}")
        return result
        
    except ScraperException as e:
        error_msg = f"Scraping failed for keyword '{keywords}': {str(e)}"
        logger.error(f"[API] {error_msg}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "scraping_failed",
                "message": error_msg,
                "keyword": keywords
            }
        )
        
    except Exception as e:
        error_msg = f"Unexpected error for keyword '{keywords}': {str(e)}"
        logger.error(f"[API] {error_msg}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "internal_error",
                "message": error_msg,
                "keyword": keywords
            }
        )

@app.get(
    "/search/naver_popular",
    summary="ë„¤ì´ë²„ ì¸ê¸°ì£¼ì œ ìŠ¤í¬ë˜í•‘",
    description="ë„¤ì´ë²„ ê²€ìƒ‰ ì¸ê¸°ì£¼ì œë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ HTTP 500 ì—ëŸ¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_model=None
)
async def popular(keywords: str):
    """ë„¤ì´ë²„ ì¸ê¸°ì£¼ì œ ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
        
    Returns:
        {'keyword': str, 'result': [{'rank': int, 'keyword': str}, ...]}
        
    Raises:
        HTTPException: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ 500 ì—ëŸ¬
    """
    logger.info(f"[API] Received request for naver_popular: {keywords}")
    
    try:
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(executor, naver_popular, keywords)
        logger.info(f"[API] Successfully completed naver_popular: {keywords}")
        return result
        
    except ScraperException as e:
        error_msg = f"Scraping failed for keyword '{keywords}': {str(e)}"
        logger.error(f"[API] {error_msg}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "scraping_failed",
                "message": error_msg,
                "keyword": keywords
            }
        )
        
    except Exception as e:
        error_msg = f"Unexpected error for keyword '{keywords}': {str(e)}"
        logger.error(f"[API] {error_msg}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "internal_error",
                "message": error_msg,
                "keyword": keywords
            }
        )
    
@app.get(
    "/search/naver_together",
    summary="ë„¤ì´ë²„ í•¨ê»˜ì°¾ì€ í‚¤ì›Œë“œ ìŠ¤í¬ë˜í•‘",
    description="ë„¤ì´ë²„ ê²€ìƒ‰ í•¨ê»˜ì°¾ì€ í‚¤ì›Œë“œë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤ (Selenium ì‚¬ìš©). ì‹¤íŒ¨ ì‹œ HTTP 500 ì—ëŸ¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_model=None
)
async def together(keywords: str):
    """ë„¤ì´ë²„ í•¨ê»˜ì°¾ì€ í‚¤ì›Œë“œ ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
        
    Returns:
        {'keyword': str, 'result': [{'rank': int, 'keyword': str}, ...]}
        
    Raises:
        HTTPException: ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ 500 ì—ëŸ¬
    """
    logger.info(f"[API] Received request for naver_together: {keywords}")
    
    try:
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(executor, naver_together, keywords)
        logger.info(f"[API] Successfully completed naver_together: {keywords}")
        return result
        
    except ScraperException as e:
        error_msg = f"Scraping failed for keyword '{keywords}': {str(e)}"
        logger.error(f"[API] {error_msg}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "scraping_failed",
                "message": error_msg,
                "keyword": keywords
            }
        )
        
    except Exception as e:
        error_msg = f"Unexpected error for keyword '{keywords}': {str(e)}"
        logger.error(f"[API] {error_msg}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "internal_error",
                "message": error_msg,
                "keyword": keywords
            }
        )

@app.get(
    "/health",
    summary="í—¬ìŠ¤ì²´í¬",
    description="ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"
)
async def health_check():
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "service": "naver_keyword_scraper",
        "version": "2.1.1",
        "note": "All endpoints use Selenium driver pool"
    }

@app.get(
    "/stats",
    summary="ë“œë¼ì´ë²„ í’€ í†µê³„",
    description="Selenium ë“œë¼ì´ë²„ í’€ ì‚¬ìš© í†µê³„ í™•ì¸"
)
async def get_stats():
    """ë“œë¼ì´ë²„ í’€ í†µê³„ ì—”ë“œí¬ì¸íŠ¸"""
    pool = get_driver_pool()
    stats = pool.get_stats()
    
    return {
        "driver_pool_stats": stats,
        "description": {
            "total_requests": "ì´ Selenium ìš”ì²­ ìˆ˜",
            "driver_restarts": "ë“œë¼ì´ë²„ ì¬ì‹œì‘ íšŸìˆ˜",
            "driver_errors": "ë“œë¼ì´ë²„ ì—ëŸ¬ ë°œìƒ íšŸìˆ˜"
        }
    }

@app.get(
    "/",
    summary="API ì •ë³´",
    description="API ê¸°ë³¸ ì •ë³´"
)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "Naver Keyword Scraper API",
        "version": "2.1.1",
        "endpoints": {
            "related": "/search/naver_related?keywords={keyword}",
            "popular": "/search/naver_popular?keywords={keyword}",
            "together": "/search/naver_together?keywords={keyword}",
            "health": "/health",
            "stats": "/stats"
        },
        "features": [
            "ğŸš€ ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ Selenium ë“œë¼ì´ë²„ í’€ ì‚¬ìš©",
            "ğŸ”„ ê°•ë ¥í•œ ì¬ì‹œë„ ë¡œì§ (ì§€ìˆ˜ ë°±ì˜¤í”„)",
            "â±ï¸ íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€",
            "ğŸ”§ ì„¸ì…˜ ë³µêµ¬ ê¸°ëŠ¥",
            "ğŸš¨ ëª…í™•í•œ ì—ëŸ¬ ì‘ë‹µ (HTTP 500)",
            "ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì•ˆì „ ì •ë¦¬ ë³´ì¥",
            "â™»ï¸ ë“œë¼ì´ë²„ ìë™ ì¬ì‹œì‘ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)"
        ],
        "performance": {
            "all_endpoints": "ì—°ê´€ê²€ìƒ‰ì–´, ì¸ê¸°ì£¼ì œ, í•¨ê»˜ì°¾ì€ í‚¤ì›Œë“œ ëª¨ë‘ Selenium ì‚¬ìš©",
            "driver_pooling": "ë§¤ ìš”ì²­ë§ˆë‹¤ Chromeì„ ì—´ì§€ ì•Šê³  ì¬ì‚¬ìš©",
            "first_request": "3-10ì´ˆ (ë“œë¼ì´ë²„ ìƒì„± í¬í•¨)",
            "subsequent_requests": "2-3ì´ˆ (ë“œë¼ì´ë²„ ì¬ì‚¬ìš©)",
            "auto_restart": "100ê°œ ìš”ì²­ë§ˆë‹¤ ë“œë¼ì´ë²„ ìë™ ì¬ì‹œì‘"
        },
        "note": "ì²« ìš”ì²­ì€ ë“œë¼ì´ë²„ ìƒì„±ìœ¼ë¡œ ëŠë¦´ ìˆ˜ ìˆì§€ë§Œ, ì´í›„ ìš”ì²­ì€ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤!"
    }

if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ìš©
    import asyncio
    result = asyncio.run(popular(keywords='ì œì¼ê¸°íš'))
    print(result)
